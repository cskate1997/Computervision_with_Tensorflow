{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Aircam Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "[Your Name Goes Here] DeepLearningScreenTurtleChallenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cskate1997/Computervision_with_Tensorflow/blob/main/%5BChinmay%5D_DeepLearningScreenTurtleChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAGbbBB7G4XH"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "*In this challenge you will be asked to build a deep learning solution capable of segmenting turtles on synthetic images. You will be provided with the data, which you can use to train your models; your best model will be used on a test image and you will be asked to implement some geometric algorithms based on the predictions of your model.*\n",
        "\n",
        "***Training data***\n",
        "\n",
        "The training data will consist of 30 256x256 images of the same synthetic turtle pasted on background images artificially generated by BigGAN (https://arxiv.org/abs/1809.11096).\n",
        "\n",
        "Feel free to increase the dataset size as needed and make any relevant changes to the dataset creation. \n",
        "\n",
        "Each image will come with the ground truth per-pixel segmentation mask which you can leverage for your training setup. We strongly recommend that you treat the provided task as semantic segmentation with 2 classes -- foreground (turtle) and background (everything else).\n",
        "\n",
        "You are also allowed to use external data sources and pre-trained weights, but please provide justification if you choose to do so.\n",
        "\n",
        "\n",
        "***Test data***\n",
        "\n",
        "There will only be a single test image without the provided GT.\n",
        "\n",
        "The test image differs from the training data and it is up to you to decide how to approach these differences. Notably, the test image is of resolution 512x512 and your predicted mask must be of the same resolution.\n",
        "\n",
        "\n",
        "***Tasks***\n",
        "\n",
        "1. Your main task is to build a deep learning model capable of accurately segmenting the turtle in the test image.\n",
        "2. Based on the segmentation mask predicted by your model, you will need to implement an algorithm that finds a convex hull, i.e. a polygon enclosing all the foreground (i.e. turtle) pixels.\n",
        "3. [Bonus Points] Implement an algorithm that calculates the area of the polygon from the result of task 2. \n",
        "\n",
        "***If you are using third-party code, you have to provide explanation of why you need that code and what that code does. We evaluate your submission based on the code you have written and if there is no such code, we won't be able to evaluate and proceed to the next stage.***\n",
        "\n",
        "***Rules***\n",
        "\n",
        "* While we provide all the code in PyTorch, feel free to use other deep learning frameworks as needed\n",
        "* Feel free to use all the imported Python libraries\n",
        "* For tasks 2 and 3 ***you are not allowed*** to use third-party functions that readily solve those tasks, e.g. you are not allowed to use various `cv2` and `scikit-image` operators. We expect the algorithms to be based on points and geometry rather than full-image operations.\n",
        "\n",
        "\n",
        "***Submission***\n",
        "\n",
        "* ***You must send us only a single link to the Colab notebook with your solution and nothing else!*** We should be able to reproduce your results by running the notebook. If you require additional files, use `gdown` to download them into the session storage (see Task 1 for details).\n",
        "* Include your comments and explanations on the decisions that you made to approach the problem;\n",
        "* Make sure to include the estimate of approximately how much time it took you to get to the final solution.\n",
        "\n",
        "***Colab setup***\n",
        "\n",
        "* To use GPU, go to `Runtime -> Change Runtime Type -> GPU`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to install pytorch pretrained biggan\n",
        "pip install pytorch_pretrained_biggan"
      ],
      "metadata": {
        "id": "mbM4tM8iiKQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj5E0buPG4XL"
      },
      "source": [
        "# comment the following line if you are working outside of a notebook environment\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LqTiX6Pmvsx"
      },
      "source": [
        "# Used to download any files you need for your solution from Google Drive\n",
        "import gdown\n",
        "gdown.download(\"https://drive.google.com/uc?id=1ymKI8M73kBIck2b7S7QG02aiP4yXLaML\", \"turtle.png\", quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1igYerlCG4XM"
      },
      "source": [
        "# read and visualise the turtle image\n",
        "turtle_image = Image.open('./turtle.png')\n",
        "# it is a 4-channel RGB+Alpha image of size 2394x1800\n",
        "print(turtle_image.mode, turtle_image.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a4S2KZsG4XM"
      },
      "source": [
        "turtle_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGx2IWMwG4XN"
      },
      "source": [
        "# to create the training set, we will resize the turtle image to 256x256\n",
        "turtle_image_256x256 = turtle_image.resize((256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD5uUP_4G4XN"
      },
      "source": [
        "# Background Images\n",
        "\n",
        "As written above, we will use a generative adversarial network called \"BigGAN\" pre-trained on ImageNet to create a set of background images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaq5oa1CG4XN"
      },
      "source": [
        "# first, we need to install the python package called `pytorch_pretrained_biggan` (https://github.com/huggingface/pytorch-pretrained-BigGAN)\n",
        "# if in the notebook environment, please uncomment the following line to install this package\n",
        "# !pip install pytorch_pretrained_biggan\n",
        "# there might be some errors related to pip's dependency resolver which you can safely ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of53GoI8G4XO"
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_biggan import (\n",
        "    BigGAN,\n",
        "    truncated_noise_sample,\n",
        "    convert_to_images,\n",
        "    one_hot_from_int,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBmJTXHxG4XO"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# boilerplate pytorch code enforcing reproducibility\n",
        "torch.manual_seed(42)\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxPnP4h8G4XP"
      },
      "source": [
        "BigGAN is a memory-intensive network.\n",
        "\n",
        "To save time and memory, we will only generate 30 different background images.\n",
        "Feel free to change this setup as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHnvY_AjG4XP"
      },
      "source": [
        "# load the 256x256 model\n",
        "model = BigGAN.from_pretrained('biggan-deep-256').to(device).eval()\n",
        "\n",
        "# every time we will run with batch size of 3 in order to not run out of memory\n",
        "num_passes = 10\n",
        "batch_size = 3\n",
        "\n",
        "# default noise value from the provided repository\n",
        "truncation = 0.4\n",
        "\n",
        "background_images = []\n",
        "\n",
        "for _ in range(num_passes):\n",
        "    # BigGAN uses imagenet and hence each time we will choose one of 1000 categories\n",
        "    class_vector = torch.from_numpy(\n",
        "        one_hot_from_int(np.random.randint(0, 1000, size=batch_size), batch_size=batch_size)\n",
        "    ).to(device)\n",
        "    noise_vector = torch.from_numpy(\n",
        "        truncated_noise_sample(truncation=truncation, batch_size=batch_size)\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate the images and convert them to PIL image\n",
        "    with torch.no_grad():\n",
        "        output = model(noise_vector, class_vector, truncation).cpu()\n",
        "        background_images.extend(convert_to_images(output))\n",
        "\n",
        "# We won't need the GAN model anymore,\n",
        "# so we can safely delete it and free up some memory\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRd_CZHXG4XQ"
      },
      "source": [
        "# Let's see how one of the images look like\n",
        "random.choice(background_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8OBi3LrG4XQ"
      },
      "source": [
        "# Training Set\n",
        "Given 30 background images and the turtle image, we will paste the turtle onto the background images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDlSOnMYG4XR"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iuJOPcXG4XR"
      },
      "source": [
        "tensor_transform = transforms.ToTensor()\n",
        "\n",
        "def random_paste(background_image, turtle_image, min_scale=0.25, max_scale=0.65):\n",
        "    \"\"\"Randomly scales and pastes the turtle image onto the background image\"\"\"\n",
        "    \n",
        "    w, h = turtle_image.size\n",
        "    # first, we will randomly downscale the turtle image\n",
        "    new_w = int(random.uniform(min_scale, max_scale) * w)\n",
        "    new_h = int(random.uniform(min_scale, max_scale) * h)\n",
        "    resized_turtle_image = turtle_image.resize((new_w, new_h))\n",
        "\n",
        "    # second, will randomly choose the locations where to paste the new image\n",
        "    start_w = random.randint(0, w - new_w)\n",
        "    start_h = random.randint(0, h - new_h)\n",
        "\n",
        "    # third, will create the blank canvas of the same size as the original image\n",
        "    canvas_image = Image.new('RGBA', (w, h))\n",
        "\n",
        "    # and paste the resized turtle onto it, preserving the mask\n",
        "    canvas_image.paste(resized_turtle_image, (start_w, start_h), resized_turtle_image)\n",
        "    \n",
        "    # Turtle image is of mode RGBA, while background image is of mode RGB;\n",
        "    # `.paste` requires both of them to be of the same type.\n",
        "    background_image = background_image.copy().convert('RGBA')\n",
        "    # finally, will paste the resized turtle onto the background image\n",
        "    background_image.paste(resized_turtle_image, (start_w, start_h), resized_turtle_image)\n",
        "    return background_image, canvas_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvW679jeG4XS"
      },
      "source": [
        "training_set = []  # image, segmentation mask\n",
        "\n",
        "for background_image in background_images:\n",
        "  # paste the turtle onto background image\n",
        "  aug_image, aug_mask = random_paste(background_image.copy(), turtle_image_256x256.copy())\n",
        "  # convert PIL images to pytorch tensors\n",
        "  training_pair = [\n",
        "      tensor_transform(aug_image)[:3],  # keep the rgb only\n",
        "      # For the mask, we only need the last (4th) channel,\n",
        "      # and we will encode the mask as boolean\n",
        "      tensor_transform(aug_mask)[-1:] > 0,\n",
        "  ]\n",
        "  training_set.append(training_pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6vNFmxEG4XS"
      },
      "source": [
        "# Let's visualise some subset of the training set\n",
        "sample_indices = np.random.choice(len(training_set), size=9, replace=False)\n",
        "sample_images = []\n",
        "sample_masks = []\n",
        "for i in sample_indices:\n",
        "    image, mask = training_set[i]\n",
        "    sample_images.append(image)\n",
        "    sample_masks.append(mask)\n",
        "    \n",
        "plt.figure(figsize=(18, 18))\n",
        "plt.subplot(121)\n",
        "plt.imshow(torchvision.utils.make_grid(sample_images, nrow=3).permute(1, 2, 0).cpu().numpy())\n",
        "plt.subplot(122)\n",
        "plt.imshow(torchvision.utils.make_grid(sample_masks, nrow=3).permute(1, 2, 0).float().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Z2fObCG4XT"
      },
      "source": [
        "# Test Image\n",
        "Now, let's load the test image. As mentioned above, it is of a slightly higher 512x512 resolution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Also load below Test image and it's dependencies during testing and polygon plotting"
      ],
      "metadata": {
        "id": "JnjSjM1tiqbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXnxFtHMG4XU"
      },
      "source": [
        "gdown.download(\"https://drive.google.com/uc?id=1_55KX8AK8pZ936Zv27t8Q-ZY8BJjuBqa\", \"test.png\", quiet=False)\n",
        "test_image = Image.open('./test.png')\n",
        "# it is a 3-channel RGB image of size 512x512\n",
        "print(test_image.mode, test_image.size)\n",
        "test_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "temA8dgIVUtB"
      },
      "source": [
        "# Task 1: Predicting segmentation mask\n",
        "\n",
        "*This is where you need to implement your deep learning solution. Your approach should output a result at the native 512x512 resolution of the test image.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8deu6D6VVSCK"
      },
      "source": [
        "# # TODO: Implement and train the deep model\n",
        "\n",
        "# # TODO: Save the model weights and upload them to Google Drive\n",
        "\n",
        "# load_model_weights = False\n",
        "# if load_model_weights:\n",
        "#     # After uploading your saved model weights to Google Drive, share to\n",
        "#     # \"Anyone with the link\" and extract FILE_ID from the share link\n",
        "#     # See https://support.google.com/drive/answer/2494822?hl=en&co=GENIE.Platform%3DDesktop\n",
        "#     # for more information\n",
        "#     # Now the weights can be downloaded and used via gdown:\n",
        "#     saved_model_url = \"https://drive.google.com/uc?id=FILE_ID\"\n",
        "#     gdown.download(saved_model_url, \"saved_model.pth\", quiet=True)\n",
        "\n",
        "#     # TODO: Load your saved model weights e.g. torch.load(\"saved_model.pth\")\n",
        "\n",
        "# test_image_tensor = tensor_transform(test_image)\n",
        "\n",
        "# def get_mask_from_image(test_image):\n",
        "#   # TODO: Use the deep model that predicts the segmentation mask on the test image\n",
        "#   # The model with the saved weights should be used, if load_model_weights is True\n",
        "#   test_mask = test_image.mean(0) < 0.5\n",
        "#   return test_mask.byte()\n",
        "\n",
        "# test_mask_tensor = get_mask_from_image(test_image_tensor)\n",
        "\n",
        "# plt.figure(figsize=(12, 12))\n",
        "# plt.subplot(121)\n",
        "# plt.imshow(test_image_tensor.numpy().transpose(1, 2, 0))\n",
        "# plt.subplot(122)\n",
        "# plt.imshow(test_mask_tensor.numpy(), cmap=\"gray\", vmin=0, vmax=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the libraries for predicting\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.models.segmentation\n",
        "import torch\n",
        "import torchvision.transforms as tf\n",
        "\n",
        "\n",
        "Learning_Rate=1e-5\n",
        "width=height=512 # image width and height\n",
        "batchSize=3"
      ],
      "metadata": {
        "id": "TxwqPuYWjZug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms\n",
        "\n",
        "transformImg=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)), tf.ToTensor(),tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "transformAnn=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)), tf.ToTensor()])"
      ],
      "metadata": {
        "id": "wfyVnmhbji_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageOps\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "#Convert image and mask to it's format\n",
        "def ReadRandomImage():\n",
        "  indx=np.random.randint(0,len(training_set))\n",
        "  Img, mask = training_set[indx]\n",
        "  mask = mask.bool().int()\n",
        "  mask = mask.cpu().detach().numpy()\n",
        "  mask = mask.transpose((1,2,0)).squeeze()\n",
        "  Img = Img.cpu().detach().numpy()\n",
        "  Img = Img.transpose((1,2,0))\n",
        "  Img = Img.astype(np.float64) / Img.max()\n",
        "  Img = 255 * Img\n",
        "  img = Img.astype(np.uint8)\n",
        "  # msk = T.ToPILImage()(mask.to('cpu'))\n",
        "  # # gray_image = ImageOps.grayscale(msk)\n",
        "  # im1 = msk.save(\"gk.png\", format= \"PNG\")\n",
        "  AnnMap = np.zeros(img.shape[0:2], np.float32)\n",
        "  if mask is not None: AnnMap[mask == 1] = 1\n",
        "  Img = transformImg(img)\n",
        "  AnnMap = transformAnn(AnnMap)\n",
        "\n",
        "  return Img, AnnMap"
      ],
      "metadata": {
        "id": "eJwwBI8OkOaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadBatch(): # Load batch of images\n",
        "    images = torch.zeros([batchSize,3,height,width])\n",
        "    ann = torch.zeros([batchSize, height, width])\n",
        "    \n",
        "    for i in range(batchSize):\n",
        "        images[i],ann[i]=ReadRandomImage()\n",
        "    \n",
        "    return images, ann"
      ],
      "metadata": {
        "id": "VFkRdtAzkuIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if device can use GPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "#model\n",
        "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
        "Net.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 2 classes\n",
        "Net=Net.to(device)\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer"
      ],
      "metadata": {
        "id": "xt99-AGukxjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**\n",
        "\n"
      ],
      "metadata": {
        "id": "IswzbdSilLz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "for itr in range(10000): # Training loop\n",
        "   images,ann=LoadBatch() \n",
        "   \n",
        "   images=torch.autograd.Variable(images,requires_grad=False).to(device)    \n",
        "   \n",
        "   ann = torch.autograd.Variable(ann,requires_grad=False).to(device)              \n",
        "   \n",
        "   Pred=Net(images)['out'] # make prediction\n",
        "   Net.zero_grad()\n",
        "   \n",
        "   #Loss Function\n",
        "   criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
        "   Loss=criterion(Pred,ann.long()) # Calculate cross entropy loss\n",
        "   Loss.backward() # Backpropogate loss\n",
        "   optimizer.step() # Apply gradient descent change to weight\n",
        "   seg = torch.argmax(Pred[0], 0).cpu().detach().numpy()  # Get prediction classes\n",
        "   print(itr,\") Loss=\",Loss.data.cpu().numpy())\n",
        "   \n",
        "   if itr % 500 == 0: #Save model weight once every 500 steps permenant file\n",
        "        print(\"Saving Model\" +str(itr) + \".torch\")\n",
        "        torch.save(Net.state_dict(),   str(itr) + \".torch\")"
      ],
      "metadata": {
        "id": "kSfxh3SalEvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Saved model weights**"
      ],
      "metadata": {
        "id": "HLxw6Dq6l0qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "# https://drive.google.com/file/d/1xPxTY4n_cMabEJcd8wtIfmf0YnDoLyQl/view?usp=sharing\n",
        "saved_model_url = \"https://drive.google.com/uc?id=1xPxTY4n_cMabEJcd8wtIfmf0YnDoLyQl/\"\n",
        "gdown.download(saved_model_url, \"saved_model4_9500.torch\", quiet=True)"
      ],
      "metadata": {
        "id": "IGMo5gYClyML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torchvision.models.segmentation\n",
        "import torch\n",
        "import torchvision.transforms as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "modelPath = \"9500.torch\"  # Path to trained model\n",
        "imagePath = \"./test.png\"  # Test image\n",
        "height=width=512\n",
        "\n",
        "transformImg = tf.Compose([tf.ToPILImage(), tf.Resize((height, width)), tf.ToTensor(),tf.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))])  \n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  \n",
        "\n",
        "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)  # Load net\n",
        "Net.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))  # Change final layer to 2 classes\n",
        "Net = Net.to(device)  # Set net to GPU or CPU\n",
        "Net.load_state_dict(torch.load(modelPath)) # Load trained model\n",
        "\n",
        "#Evaluation\n",
        "Net.eval() # Set to evaluation mode\n",
        "Img = cv2.imread(imagePath) # load test image\n",
        "\n",
        "\n",
        "height_orgin , widh_orgin ,d = Img.shape # Get image original size \n",
        "plt.imshow(Img[:,:,::-1])  # Show image\n",
        "plt.show()\n",
        "Img = transformImg(Img)  # Transform to pytorch\n",
        "Img = torch.autograd.Variable(Img, requires_grad=False).to(device).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    Prd = Net(Img)['out']  # Run net\n",
        "Prd = tf.Resize((height_orgin,widh_orgin))(Prd[0]) # Resize to origninal size\n",
        "seg = torch.argmax(Prd, 0).cpu().detach().numpy()  # Get  prediction classes\n",
        "plt.imshow(seg)  # display image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8tYU6F8fmDEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mask_tensor = tensor_transform(seg)\n",
        "test_image_tensor = tensor_transform(test_image)"
      ],
      "metadata": {
        "id": "uH-4eP5dmXLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaYvZdUKu_sQ"
      },
      "source": [
        "# Task 2: Calculating tight enclosing polygon from segmentation mask\n",
        "\n",
        "*This is where you need to implement your algorithm that predicts a convex hull, an enclosing polygon of foreground pixels. You are not allowed to use cv2, scikit-image or other libraries' functionality that readily solve this task. Treat this problem as point-based rather than the image-based one.*\n",
        "\n",
        "*You don't have to use PyTorch for this part. Your approach should output a result at the native 512x512 resolution of the test image.* \n",
        "\n",
        "*For the purposes of this assignment, O(n^2) is considered a good time complexity*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cmp_to_key\n",
        "\n",
        "#Class Pixel\n",
        "class Pixel:\n",
        "  def __init__(self, x = None, y = None):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "pi0 = Pixel(0,0)\n",
        "\n",
        "def sec_top(S):\n",
        "  return S[-2]\n",
        "def sqrdist(pi1,pi2):\n",
        "  return ((pi1.x - pi2.x)*(pi1.x - pi2.x) + (pi1.y - pi2.y)*(pi1.y - pi2.y))\n",
        "def orientation(p, q, r):\n",
        "    val = ((q.y - p.y) * (r.x - q.x) -\n",
        "           (q.x - p.x) * (r.y - q.y))\n",
        "    if val == 0:\n",
        "        return 0  # collinear\n",
        "    elif val > 0:\n",
        "        return 1  # clock wise\n",
        "    else:\n",
        "        return 2  # counterclock wise\n",
        "def compare(pi1, pi2):\n",
        "    # Find orientation\n",
        "    o = orientation(pi0, pi1, pi2)\n",
        "    if o == 0:\n",
        "        if sqrdist(pi0, pi2) >= sqrdist(pi0, pi1):\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        if o == 2:\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "def convexHull(points, n):\n",
        "   \n",
        "    # Find the bottommost point\n",
        "    ymin = points[0].y\n",
        "    min = 0\n",
        "    for i in range(1, n):\n",
        "        y = points[i].y\n",
        " \n",
        "        # Pick the bottom-most or choose the left\n",
        "        # most point in case of tie\n",
        "        if ((y < ymin) or\n",
        "            (ymin == y and points[i].x < points[min].x)):\n",
        "            ymin = points[i].y\n",
        "            min = i\n",
        " \n",
        "    # Place the bottom-most point at first position\n",
        "    points[0], points[min] = points[min], points[0]\n",
        "    p0 = points[0]\n",
        "    points = sorted(points, key=cmp_to_key(compare))\n",
        "    for i in range(1, n):\n",
        "        while ((i < n - 1) and\n",
        "        (orientation(p0, points[i], points[i + 1]) == 0)):\n",
        "            i += 1\n",
        " \n",
        "        points[m] = points[i]\n",
        "        m += 1  # Update size of modified array\n",
        "    if m < 3:\n",
        "        return\n",
        "\n",
        "    S = []\n",
        "    S.append(points[0])\n",
        "    S.append(points[1])\n",
        "    S.append(points[2])\n",
        "    # Process remaining n-3 points\n",
        "    for i in range(3, m):\n",
        "        while ((len(S) > 1) and\n",
        "        (orientation(sec_top(S), S[-1], points[i]) != 2)):\n",
        "            S.pop()\n",
        "        S.append(points[i])\n",
        "    lst = []\n",
        "    while S:\n",
        "        p = S[-1]\n",
        "        lst.append([p.x,p.y])\n",
        "        S.pop()\n",
        "    goat = tensor_transform(np.asarray(lst)).squeeze()\n",
        "    return goat"
      ],
      "metadata": {
        "id": "xQrSQaGkmdoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uUM0_MCjnReQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2fUBrrqvYO-"
      },
      "source": [
        "def get_tight_polygon_from_mask(test_mask):\n",
        "  # TODO: Implement an algorithm that computes the enclosing polygon from the segmentation mask.\n",
        "  mask_points_n2 = torch.stack(torch.where(test_mask_tensor == 1), 1)\n",
        "  a = mask_points_n2.tolist()\n",
        "  new_mask_points = []\n",
        "  for i in a:\n",
        "    new_mask_points.append(tuple(i[1:]))\n",
        "  points = []\n",
        "  for point in new_mask_points:\n",
        "    points.append(Pixel(point[0],point[1]))\n",
        "\n",
        "  n = len(points)\n",
        "  polygon_points_n2 = convexHull(points,n)\n",
        "  return dummy_polygon_points_n2\n",
        "\n",
        "def visualize_polygon_on_image(test_image, polygon_points_n2):\n",
        "  # append first point to close the figure\n",
        "  polygon_points_n2 = torch.cat([polygon_points_n2, polygon_points_n2[:1]], 0)\n",
        "  ys, xs = torch.split(polygon_points_n2, 1, dim=-1)  \n",
        "  plt.figure(figsize=(12, 12))\n",
        "  plt.imshow(test_image.numpy().transpose(1, 2, 0))\n",
        "  plt.plot(xs.numpy(), ys.numpy()) \n",
        "\n",
        "\n",
        "polygon_points_n2_tensor = get_tight_polygon_from_mask(test_mask_tensor)\n",
        "visualize_polygon_on_image(test_image_tensor, polygon_points_n2_tensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7actVPlIzKKv"
      },
      "source": [
        "# Task 3 [bonus points]: Calculating the area of the polygon\n",
        "*This is where you need to implement your area calculation algorithm. You are not allowed to use cv2, scikit-image or other libraries' functionality that readily solve this task. Once again, treat this problem as a point-based rather than the image-based one.*\n",
        "\n",
        "*You don't have to use PyTorch for this part. Your approach should output a result at the native 512x512 resolution of the test image.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvVZ61S8zbdw"
      },
      "source": [
        "def calculate_polygon_area(polygon_points_n2):\n",
        "  # TODO: Implement the algorithm\n",
        "  area = 0.0\n",
        "  return area\n",
        "\n",
        "print(\"Area = {:.4f}\".format(calculate_polygon_area(polygon_points_n2_tensor)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}